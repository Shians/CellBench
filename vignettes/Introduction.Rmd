---
title: "Introduction to CellBench"
author: "Shian Su"
date: "`r Sys.Date()`"
output: BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(CellBench)
library(dplyr)
library(purrr)
library(ggplot2)
```


```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.retina = 1
)
```

# Introduction

CellBench is a package to assist with creating benchmarks for single cell analysis methods. We provide functions for working with `SingleCellExperiments` objects and a framework for constructing benchmarks for different single cell datasets across different methods or combinations of methods.

The aim of this package is to make it simpler for developers to construct combinatorial designs and provide a flat data structure to store the organised outputs of analysis methods. We provide some fully constructed benchmarking pipelines for a set of single-cell benchmark datasets, and we hope that the framework will allow users to easily construct benchmarks in an organised and expressive manner.

# Quick start

There are 3 fundamental components to the benchmarks in this package, `list`s of data, `list`s of functions and a `tibble` with a `list-column` that we will call a `benchmark_tbl`. For simplicity we use randomly generated data and simple functions, but hopefully it's clear how the idea extends into more complex functions and benchmarks.

```{r}
library(CellBench)
library(dplyr)
library(purrr)
library(ggplot2)

# ensure that a vector has 0 mean
center <- function(x) {
    x - mean(x)
}

# we start with random vectors
datasets <- list(
    set1 = rnorm(500, mean = 2, sd = 1),
    set2 = rnorm(500, mean = 1, sd = 2)
)

# we then have a "method" that adds additional noise
add_noise <- list(
    none = identity, # included to provide a baseline for comparison
    add_bias = function(x) { x + 1 },
    add_variance = function(x) { x + center(rnorm(500, mean = 0, sd = 2)) }
)
```

Now we have 2 `list`s.

* A `list` of datasets. In this context it is two randomly generated vectors, but it can be any arbitrary object. Ideally all objects in the list are of the same type, this makes it success more likely when when the same methods are applied across all datasets.
* A `list` of functions. These are the functions that will perform one step of our pipeline stored neatly in a single object. For this example we have methods of adding noise to our sample, the first function simply returns the data as-is which is helpful for comparison. The second adds a bias by shifting all values by 1 and the third adds random noise with a standard deviation of 2.

```{r}
# this allows us to construct the first `benchmark_tbl`
res1 <- apply_methods(datasets, add_noise)
res1
```

So we see that we have a `result` for every combination of `data` and `add_noise` method applied. We can then summarise this into metrics.

```{r}
metric <- list(
    mean = mean,
    sd = sd
)

res2 <- apply_methods(res1, metric)
res2
```

Now the `result` column has been augmented to reflect the metric produced from each metric summary of each result from the previous table. Thus it is simple to generate combinatorial benchmarking schemes simply by successively applying further `list`s of functions.

```{r, fig.retina = 1}
# center ggplot titles and increase text size
ggplot2::theme_update(plot.title = element_text(hjust = 0.5, size = 20))

res2 %>%
    dplyr::filter(add_noise %in% c("add_bias", "none")) %>%
    ggplot(aes(x = data, y = result, group = add_noise, fill = add_noise)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_grid(~metric) +
    ggtitle("Effect of adding bias")

res2 %>%
    dplyr::filter(add_noise == "add_variance" | add_noise == "none") %>%
    ggplot(aes(x = data, y = result, group = add_noise, fill = add_noise)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_grid(~metric) +
    ggtitle("Effect of adding variance")
```

Then for a quick summary of what pipeline has been performed you can use `summary()` on the results table.

```{r}
summary(res2)
```

# Key objects and concepts

## Function Piping

In this package many examples make heavy use of the pipe operator `%>%` from [magrittr](https://magrittr.tidyverse.org). This is useful for writing cleaner code that is easier to debug.

```{r, eval = FALSE}
# the following two statements are equivalent
f(x)
x %>% f()

# as are these
f(x, y)
x %>% f(y)

# and these
h(g(f(x)))
x %>% f() %>% g() %>% h()

# or these
h(g(f(x, a), b), c)
x %>% f(a) %>% g(b) %>% h(c)
```

We can see in the last example that with many functions composed together, the piped form reads from left to right and it's clear which arguments belong to which function, whereas in the nested form it is more difficult to clearly identify what is happening. In general piping data into a function calls the function with the data serving as the first argument, more complex behaviour can be achieved and is describe on the [magrittr](https://magrittr.tidyverse.org) web page.

## Mapping or List-apply

Lists in R are containers for a collection of arbitrary objects. In this package we encourage users to use lists as containers for a series of identically-typed objects, using them as if they were vectors for data types that vectors cannot contain. For example we store our datasets in lists of SingleCellExperiment objects and analysis methods in lists of functions, these data types would not be accepted within a vector.

To work with lists we encourage using `lapply` or `purrr::map`, these allow functions to be applied to each element of a list and return the result in a list.

```{r}
x <- list(
    a = 1,
    b = 2,
    c = 3
)

lapply(x, sqrt)
```

## List of datasets

The benchmarking workflow starts with a list of datasets, even if you only have one dataset you will need to store it in a list for workflow to function. In our example the dataset was a list of 2 randomly generated vectors.

```{r, eval = FALSE}
# we use a list of random vectors
datasets <- list(
    set1 = rnorm(500, mean = 2, sd = 1),
    set2 = rnorm(500, mean = 1, sd = 2)
)

# could have been any other kind of object as long as they are consistent
datasets <- list(
    set1 = matrix(rnorm(500, mean = 2, sd = 1), ncol = 5, nrow = 10),
    set2 = matrix(rnorm(500, mean = 2, sd = 1), ncol = 5, nrow = 10)
)
```

Any kind of object can be stored in a list, so there is great flexibility in what kind of starting point can be used for the benchmarking workflow.

## List of functions

In R functions themselves are a type of object, so they too can be stored in lists, this may be unfamiliar to most people but this allows very simple addition of methods.

```{r, eval = FALSE}
# "identity" is a function, and we are assigning to a new name
# the remaining two elements of the list are just regular function definitions
add_noise <- list(
    none = identity, # included to provide a baseline for comparison
    add_bias = function(x) { x + 1 },
    add_variance = function(x) { x + center(rnorm(500, mean = 0, sd = 2)) }
)
```

The key thing to note is that the function must be callable and take a single argument. This may mean you need to write a wrapper function or use `purrr::partial()` to fill in some arguments. For example both `mean` and `sd` have `na.rm` arguments, because the element of the list must itself be a function, simply writing something like `mean(na.rm = TRUE)` will not work, as it is an incomplete function call. Instead we have two main options:

```{r, eval = FALSE}
# using anonymous function wrappers
metric <- list(
    mean = function(x) { mean(x, na.rm = TRUE) },
    sd = function(x) { sd(x, na.rm = TRUE) }
)

# using purrr partial function
partial <- purrr::partial # explicit namespacing to avoid ambiguity
metric <- list(
    mean = partial(mean, na.rm = TRUE),
    sd = partial(sd, na.rm = TRUE)
)

# example use with kmeans
clustering <- list(
    kmeans_4 = partial(kmeans, centers = 4),
    kmeans_5 = partial(kmeans, centers = 5),
    kmeans_6 = partial(kmeans, centers = 6)
)
```

`purrr::partial()` is known as partial-application of a function: it takes a function and arguments to that function, then returns a new function that is the function with the provided arguments filled in. This is slightly more explicit than creating the function wrapper, since the function wrapper can perform many more tasks within its body than just setting arguments, whereas `purrr::partial()` makes it clear all you're doing is setting some arguments.

## Benchmark tibble and list-columns

The `benchmark_tbl` is a very light wrapper around the standard tibble provided by `tibble::tibble()`. This is like a regular `data.frame()` except it has some pretty printing features that are particularly useful for [list-columns](https://jennybc.github.io/purrr-tutorial/ls13_list-columns.html). A list column is a special type of column where the values are not atomic, i.e. cannot be stored in a vector. This allows arbitrary data types to be stored in a column but with the caveat that pulling out that column returns a list rather than a vector. This has implications for how to perform mutations using `dplyr` verbs and in general will not behave expectedly with vectorised functions.

In the framework established by this package, the first column will be the name of the data, followed by columns specifying the names of the analysis steps and ending with a list-column containing the result of the specified dataset after processing by the chain of analysis methods.

```{r}
class(res2)
```

Because they are tibbles, they respond well to `dplyr` verbs, or most regular `data.frame` manipulations.

```{r}
res2 %>% dplyr::filter(add_noise %in% c("add_variance", "none"))
```

## Applying methods

The final idea that ties together the CellBench framework is the `apply_methods()` function, which takes a `benchmark_tbl` and applies a `list` of functions. The result is that each row is processed through each method, a new column is added specifying the method applied and the result is updated to the new value.

```{r, eval = FALSE}
# create a named list of functions to apply
metric <- list(
    mean = mean,
    sd = sd
)

# each function is applied to each entry in the results column
res2 <- apply_methods(res1, metric)

# output contains rows equal to number of methods times number of input rows
res2
```

# Summary

CellBench provides a lightweight and flexible framework for working with benchmarks that have multiple steps and result in combinatorial designs for application of methods. It makes use of simple and transparent R objects that are easy to understand and manipulate, with simple function 
